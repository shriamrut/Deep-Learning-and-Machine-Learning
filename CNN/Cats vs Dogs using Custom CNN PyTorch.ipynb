{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"### Only do it for the first time\n!unzip /kaggle/input/dogs-vs-cats/train.zip ","metadata":{"execution":{"iopub.status.busy":"2023-05-06T07:17:28.860254Z","iopub.execute_input":"2023-05-06T07:17:28.861017Z","iopub.status.idle":"2023-05-06T07:25:15.302750Z","shell.execute_reply.started":"2023-05-06T07:17:28.860973Z","shell.execute_reply":"2023-05-06T07:25:15.301500Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# All imports\nimport torch\nimport torch.nn as nn\nimport matplotlib.pyplot as plt\n\nfrom PIL import Image\nimport os\nimport numpy as np\nfrom sklearn.model_selection import train_test_split","metadata":{"execution":{"iopub.status.busy":"2023-05-22T02:26:42.796063Z","iopub.execute_input":"2023-05-22T02:26:42.796640Z","iopub.status.idle":"2023-05-22T02:26:45.891270Z","shell.execute_reply.started":"2023-05-22T02:26:42.796590Z","shell.execute_reply":"2023-05-22T02:26:45.890083Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"# All constants here\nDIR = '/kaggle/working/train/'","metadata":{"execution":{"iopub.status.busy":"2023-05-22T02:26:45.893459Z","iopub.execute_input":"2023-05-22T02:26:45.894196Z","iopub.status.idle":"2023-05-22T02:26:45.899970Z","shell.execute_reply.started":"2023-05-22T02:26:45.894150Z","shell.execute_reply":"2023-05-22T02:26:45.898645Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"device = 'cuda' if torch.cuda.is_available() else 'cpu'","metadata":{"execution":{"iopub.status.busy":"2023-05-22T02:26:45.901994Z","iopub.execute_input":"2023-05-22T02:26:45.902479Z","iopub.status.idle":"2023-05-22T02:26:45.971403Z","shell.execute_reply.started":"2023-05-22T02:26:45.902434Z","shell.execute_reply":"2023-05-22T02:26:45.970052Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"print(device)","metadata":{"execution":{"iopub.status.busy":"2023-05-22T02:26:45.974267Z","iopub.execute_input":"2023-05-22T02:26:45.975239Z","iopub.status.idle":"2023-05-22T02:26:45.984460Z","shell.execute_reply.started":"2023-05-22T02:26:45.975189Z","shell.execute_reply":"2023-05-22T02:26:45.983065Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"cuda\n","output_type":"stream"}]},{"cell_type":"code","source":"print(f\"Number of images: {len(os.listdir(DIR))}\")","metadata":{"execution":{"iopub.status.busy":"2023-05-22T02:26:46.503596Z","iopub.execute_input":"2023-05-22T02:26:46.503994Z","iopub.status.idle":"2023-05-22T02:26:46.528120Z","shell.execute_reply.started":"2023-05-22T02:26:46.503957Z","shell.execute_reply":"2023-05-22T02:26:46.526839Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"Number of images: 25000\n","output_type":"stream"}]},{"cell_type":"code","source":"IMAGE_DIM = 128\nCHANNEL_DIM = 3","metadata":{"execution":{"iopub.status.busy":"2023-05-22T02:26:47.339172Z","iopub.execute_input":"2023-05-22T02:26:47.339924Z","iopub.status.idle":"2023-05-22T02:26:47.344721Z","shell.execute_reply.started":"2023-05-22T02:26:47.339884Z","shell.execute_reply":"2023-05-22T02:26:47.343348Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"# current goal is to work with 10k images\nfrom torchvision import transforms\nrequired_cat_count = 12000\nrequired_dog_count = 12000\ntotal_size = required_cat_count + required_dog_count \nindex = 0\ndata = torch.zeros([required_cat_count + required_dog_count, \n                    CHANNEL_DIM,IMAGE_DIM, IMAGE_DIM])\ntargets = []\nfor i,filename in enumerate(os.listdir(DIR)):\n    animal = 'cat'\n    if filename.startswith('cat'):\n        target = 0\n        if required_cat_count <= 0:\n            continue\n        required_cat_count -= 1\n    else:\n        target = 1\n        animal = 'dog'\n        if required_dog_count <= 0:\n            continue\n        required_dog_count -= 1\n    file_path = DIR + '/' + filename\n    image = Image.open(file_path).resize((IMAGE_DIM, IMAGE_DIM))\n    image_tensor = transforms.ToTensor()(image)\n    '''if index % 100 == 0:\n        plt.imshow(image_tensor.permute(1,2,0))\n        plt.title(animal)\n        plt.show()'''\n    data[index] = image_tensor.to(device)\n    index = index + 1\n    targets.append(target)","metadata":{"execution":{"iopub.status.busy":"2023-05-22T02:26:47.837548Z","iopub.execute_input":"2023-05-22T02:26:47.838560Z","iopub.status.idle":"2023-05-22T02:28:43.102573Z","shell.execute_reply.started":"2023-05-22T02:26:47.838509Z","shell.execute_reply":"2023-05-22T02:28:43.101462Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"data = data.to(device)\ntargets = torch.tensor(targets).type(torch.float32).to(device)","metadata":{"execution":{"iopub.status.busy":"2023-05-22T02:28:43.104875Z","iopub.execute_input":"2023-05-22T02:28:43.105389Z","iopub.status.idle":"2023-05-22T02:28:44.411433Z","shell.execute_reply.started":"2023-05-22T02:28:43.105345Z","shell.execute_reply":"2023-05-22T02:28:44.410363Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"print(data.shape, targets.shape)\nprint(required_cat_count, required_dog_count)","metadata":{"execution":{"iopub.status.busy":"2023-05-22T02:28:44.413142Z","iopub.execute_input":"2023-05-22T02:28:44.413536Z","iopub.status.idle":"2023-05-22T02:28:44.419442Z","shell.execute_reply.started":"2023-05-22T02:28:44.413494Z","shell.execute_reply":"2023-05-22T02:28:44.418344Z"},"trusted":true},"execution_count":9,"outputs":[{"name":"stdout","text":"torch.Size([24000, 3, 128, 128]) torch.Size([24000])\n0 0\n","output_type":"stream"}]},{"cell_type":"code","source":"def train_test_split_data(data, target, test_size=0.1, random_state=None):\n    \"\"\"\n    Splits a dataset into training and testing sets, and optionally splits the training set into mini-batches.\n\n    Args:\n        data (torch.Tensor): The input data tensor, with shape (num_samples, height, width, channels).\n        target (torch.Tensor): The target tensor, with shape (num_samples, 1).\n        test_size (float): The proportion of the data to include in the test set.\n        random_state (int): The random seed to use for shuffling the data.\n        batch_size (int): The size of the mini-batches to create from the training set.\n\n    Returns:\n        A tuple (X_train_batches, X_test_batches, y_train_batches, y_test_batches) if batch_size is not None,\n        otherwise a tuple (X_train, X_test, y_train, y_test).\n    \"\"\"\n    # Shuffle the data\n    if random_state is not None:\n        torch.manual_seed(random_state)\n    shuffled_indices = torch.randperm(data.shape[0])\n    data_shuffled = data[shuffled_indices]\n    target_shuffled = target[shuffled_indices]\n\n    # Split the shuffled dataset into training and testing sets\n    test_size = int(test_size * data.shape[0])\n    X_test = data_shuffled[:test_size]\n    y_test = target_shuffled[:test_size]\n    X_train = data_shuffled[test_size:]\n    y_train = target_shuffled[test_size:]\n    return X_train, X_test, y_train, y_test","metadata":{"execution":{"iopub.status.busy":"2023-05-22T02:28:44.422618Z","iopub.execute_input":"2023-05-22T02:28:44.423319Z","iopub.status.idle":"2023-05-22T02:28:44.433323Z","shell.execute_reply.started":"2023-05-22T02:28:44.423282Z","shell.execute_reply":"2023-05-22T02:28:44.432317Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"SPLIT = 0.2\nBATCH = 256\nSEED = 42","metadata":{"execution":{"iopub.status.busy":"2023-05-22T02:28:44.435017Z","iopub.execute_input":"2023-05-22T02:28:44.435395Z","iopub.status.idle":"2023-05-22T02:28:44.447385Z","shell.execute_reply.started":"2023-05-22T02:28:44.435358Z","shell.execute_reply":"2023-05-22T02:28:44.446305Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split_data(data, targets, SPLIT, SEED)","metadata":{"execution":{"iopub.status.busy":"2023-05-22T02:28:44.449184Z","iopub.execute_input":"2023-05-22T02:28:44.449544Z","iopub.status.idle":"2023-05-22T02:28:44.515025Z","shell.execute_reply.started":"2023-05-22T02:28:44.449507Z","shell.execute_reply":"2023-05-22T02:28:44.513976Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"# flush or remove unwanted tensors from GPU\ndel data\ndel targets\ntorch.cuda.empty_cache()","metadata":{"execution":{"iopub.status.busy":"2023-05-22T02:28:44.516470Z","iopub.execute_input":"2023-05-22T02:28:44.516848Z","iopub.status.idle":"2023-05-22T02:28:44.527441Z","shell.execute_reply.started":"2023-05-22T02:28:44.516810Z","shell.execute_reply":"2023-05-22T02:28:44.526375Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"X_train.shape, X_test.shape, y_train.shape, y_test.shape","metadata":{"execution":{"iopub.status.busy":"2023-05-22T02:28:44.528813Z","iopub.execute_input":"2023-05-22T02:28:44.529494Z","iopub.status.idle":"2023-05-22T02:28:44.542948Z","shell.execute_reply.started":"2023-05-22T02:28:44.529455Z","shell.execute_reply":"2023-05-22T02:28:44.541889Z"},"trusted":true},"execution_count":14,"outputs":[{"execution_count":14,"output_type":"execute_result","data":{"text/plain":"(torch.Size([19200, 3, 128, 128]),\n torch.Size([4800, 3, 128, 128]),\n torch.Size([19200]),\n torch.Size([4800]))"},"metadata":{}}]},{"cell_type":"code","source":"import random\n\n@torch.no_grad()\ndef get_batch(split):\n    dataset = {'train': (X_train, y_train),\n              'test': (X_test, y_test)}\n    X, y = dataset[split]\n    idx = random.randint(0, X.shape[0]-BATCH)\n    Xb,yb = X[idx:idx+BATCH], y[idx:idx+BATCH]\n    return Xb, yb","metadata":{"execution":{"iopub.status.busy":"2023-05-22T02:28:44.546582Z","iopub.execute_input":"2023-05-22T02:28:44.546866Z","iopub.status.idle":"2023-05-22T02:28:44.554499Z","shell.execute_reply.started":"2023-05-22T02:28:44.546839Z","shell.execute_reply":"2023-05-22T02:28:44.553360Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"class CNN(nn.Module):\n    def __init__(self, output_dim):\n        super(CNN, self).__init__()\n        self.feature_extraction_layers = nn.Sequential(nn.Conv2d(CHANNEL_DIM, 16, kernel_size= (3,3), padding=1), \n                                                       nn.BatchNorm2d(16), # Use batch normalization to speed up training and reduce overfitting \n                                                       nn.ReLU(), # Use ReLU activation function for faster convergence and sparsity \n                                                       nn.MaxPool2d((2,2) ,stride=2),\n                                                       \n                                                       nn.Conv2d(16, 32, kernel_size= (3,3), padding=1), \n                                                       nn.BatchNorm2d(32), nn.ReLU(), \n                                                       nn.MaxPool2d((2,2), stride=2), \n                                                       nn.Conv2d(32, 64, kernel_size= (3,3), padding=1),\n                                                       nn.BatchNorm2d(64), nn.ReLU(), \n                                                       nn.MaxPool2d((2,2), stride=2), \n                                                       nn.Conv2d(64, 128, kernel_size= (3,3), padding=1), \n                                                       nn.BatchNorm2d(128), nn.ReLU(), \n                                                       nn.MaxPool2d((2,2), stride=2),\n                                                       nn.Conv2d(128, 256, kernel_size= (3,3), padding=1), \n                                                       nn.BatchNorm2d(256), nn.ReLU(), \n                                                       nn.MaxPool2d((2,2), stride=2)) \n        self.flatten = nn.Flatten() \n        self.dropout = nn.Dropout(0.8) # Increase the dropout rate to prevent overfitting \n        self.connected_layer = nn.Linear(4096, 512) # Adjust the input dimension of the linear layer according to the output of the convolutional layers \n        self.output_layer = nn.Linear(512, output_dim)\n        \n    def forward(self, x):\n        feature_extraction_layer_output = self.feature_extraction_layers(x)\n        flatten_output = self.flatten(feature_extraction_layer_output)\n        connected_layer_output = self.connected_layer(self.dropout(flatten_output))\n        output = self.output_layer(self.dropout(connected_layer_output))\n        return output","metadata":{"execution":{"iopub.status.busy":"2023-05-22T02:28:44.558717Z","iopub.execute_input":"2023-05-22T02:28:44.559103Z","iopub.status.idle":"2023-05-22T02:28:44.575832Z","shell.execute_reply.started":"2023-05-22T02:28:44.559065Z","shell.execute_reply":"2023-05-22T02:28:44.574723Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"@torch.no_grad()\ndef evaluate(model, X, y):\n    model.eval()\n    logits = model(X)\n    probs = torch.sigmoid(logits)\n    loss = F.binary_cross_entropy(probs, y.view(-1,1))\n    return loss","metadata":{"execution":{"iopub.status.busy":"2023-05-22T01:32:31.857394Z","iopub.execute_input":"2023-05-22T01:32:31.858052Z","iopub.status.idle":"2023-05-22T01:32:31.873372Z","shell.execute_reply.started":"2023-05-22T01:32:31.858010Z","shell.execute_reply":"2023-05-22T01:32:31.872346Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"cnn = CNN(1).to(device)\nfor p in cnn.parameters():\n    p.requires_grad = True","metadata":{"execution":{"iopub.status.busy":"2023-05-22T01:32:31.874799Z","iopub.execute_input":"2023-05-22T01:32:31.875244Z","iopub.status.idle":"2023-05-22T01:32:31.920994Z","shell.execute_reply.started":"2023-05-22T01:32:31.875203Z","shell.execute_reply":"2023-05-22T01:32:31.920039Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"print(sum(p.nelement() for p in cnn.parameters()) / 1e6, \"M\")\nprint(sum(p.nelement() for p in cnn.parameters()))","metadata":{"execution":{"iopub.status.busy":"2023-05-22T01:32:31.922622Z","iopub.execute_input":"2023-05-22T01:32:31.923004Z","iopub.status.idle":"2023-05-22T01:32:31.931325Z","shell.execute_reply.started":"2023-05-22T01:32:31.922966Z","shell.execute_reply":"2023-05-22T01:32:31.930037Z"},"trusted":true},"execution_count":19,"outputs":[{"name":"stdout","text":"2.491777 M\n2491777\n","output_type":"stream"}]},{"cell_type":"code","source":"logging_interval = 1000\ninterval_average_for_plot = 1500","metadata":{"execution":{"iopub.status.busy":"2023-05-22T01:32:31.932879Z","iopub.execute_input":"2023-05-22T01:32:31.934235Z","iopub.status.idle":"2023-05-22T01:32:31.940191Z","shell.execute_reply.started":"2023-05-22T01:32:31.934191Z","shell.execute_reply":"2023-05-22T01:32:31.939045Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"code","source":"#cnn = torch.load('/kaggle/working/cnn-base-2.hdf5')","metadata":{"execution":{"iopub.status.busy":"2023-05-21T08:12:41.005219Z","iopub.execute_input":"2023-05-21T08:12:41.005658Z","iopub.status.idle":"2023-05-21T08:12:41.010800Z","shell.execute_reply.started":"2023-05-21T08:12:41.005619Z","shell.execute_reply":"2023-05-21T08:12:41.009490Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from torch.nn import functional as F\nITERATIONS = 15000\nLR = 1e-4\nWEIGHT_DECAY = 3\n\nlosses_train = []\nlosses_test = []\noptim = torch.optim.AdamW(cnn.parameters(),\n                          lr = LR,\n                         weight_decay=WEIGHT_DECAY)\n\nfor i in range(ITERATIONS):\n    Xb_train, yb_train = get_batch('train')\n    Xb_test, yb_test = get_batch('test')\n    cnn.train()\n    logits = cnn(Xb_train)\n    probs = torch.sigmoid(logits)\n    \n    '''Reset gradients to zero (old school)\n    for p in slp.parameters():\n        p.grad = None'''\n    optim.zero_grad(set_to_none=True)\n    \n    #Compute loss and back propagate\n    lossb_train = F.binary_cross_entropy(probs, yb_train.view(-1,1))\n    lossb_train.backward()\n    \n    #Test loss\n    lossb_test = evaluate(cnn, Xb_test, yb_test)\n    \n    cnn.train()\n    optim.step()\n    ''' Update gradients (old school)\n    for p in slp.parameters():\n        p.data -= LR * p.grad'''\n        \n    # Verbose logging at every iteration\n    if i%logging_interval == 0:\n        print(f\"Loss at iteration {i} - train loss: {lossb_train.item()}, test loss: {lossb_test.item()}\")\n    losses_train.append(lossb_train.item())\n    losses_test.append(lossb_test.item())","metadata":{"execution":{"iopub.status.busy":"2023-05-21T12:28:13.622625Z","iopub.execute_input":"2023-05-21T12:28:13.623351Z","iopub.status.idle":"2023-05-21T12:59:01.220319Z","shell.execute_reply.started":"2023-05-21T12:28:13.623311Z","shell.execute_reply":"2023-05-21T12:59:01.219198Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.plot(torch.tensor(losses_train).view(-1, interval_average_for_plot).mean(1))\nplt.plot(torch.tensor(losses_test).view(-1, interval_average_for_plot).mean(1))\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-05-21T12:59:01.222555Z","iopub.execute_input":"2023-05-21T12:59:01.222959Z","iopub.status.idle":"2023-05-21T12:59:01.454884Z","shell.execute_reply.started":"2023-05-21T12:59:01.222918Z","shell.execute_reply":"2023-05-21T12:59:01.453841Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### 1. CNN Base model - v1\n\n##### Saved file\n/kaggle/working/cnn-base.hdf5\n\n##### Dataset\n```\nIMAGE_DIM = 128\nCHANNEL_DIM = 3\nrequired_cat_count = 2000\nrequired_dog_count = 2000\n```\n##### Hyperparameters\n```\nITERATIONS = 300\nLR = 1e-3\nWEIGHT_DECAY = 0.8\n```\n\n##### Architecture\n```torch.manual_seed(1234)\nclass CNN(nn.Module):\n    def __init__(self, output_dim):\n        super(CNN, self).__init__()\n        self.feature_extraction_layers = nn.Sequential(nn.Conv2d(CHANNEL_DIM, 16, kernel_size= (3,3), padding=1), \n                                                       nn.BatchNorm2d(16), # Use batch normalization to speed up training and reduce overfitting \n                                                       nn.ReLU(),\n                                                       nn.MaxPool2d((2,2) ,stride=2),\n                                                       nn.Conv2d(16, 32, kernel_size= (3,3), padding=1), \n                                                       nn.BatchNorm2d(32), nn.ReLU(), \n                                                       nn.MaxPool2d((2,2), stride=2), \n                                                       nn.Conv2d(32, 64, kernel_size= (3,3), padding=1),\n                                                       nn.BatchNorm2d(64), nn.ReLU(), \n                                                       nn.MaxPool2d((2,2), stride=2), \n                                                       nn.Conv2d(64, 128, kernel_size= (3,3), padding=1), \n                                                       nn.BatchNorm2d(128), nn.ReLU(), \n                                                       nn.MaxPool2d((2,2), stride=2),\n                                                       nn.Conv2d(128, 256, kernel_size= (3,3), padding=1), \n                                                       nn.BatchNorm2d(256), nn.ReLU(), \n                                                       nn.MaxPool2d((2,2), stride=2)) \n        self.flatten = nn.Flatten() \n        self.dropout = nn.Dropout(0.8) # Increase the dropout rate to prevent overfitting \n        self.connected_layer = nn.Linear(4096, 512) # Adjust the input dimension of the linear layer according to the output of the convolutional layers \n        self.output_layer = nn.Linear(512, output_dim)\n        \n    def forward(self, x):\n        feature_extraction_layer_output = self.feature_extraction_layers(x)\n        flatten_output = self.flatten(feature_extraction_layer_output)\n        connected_layer_output = self.connected_layer(self.dropout(flatten_output))\n        output = self.output_layer(self.dropout(connected_layer_output))\n        return output\n```\n#### Model Learning history\n* Loss at iteration 0 - train loss: 1.111029028892517, test loss: 0.6931840181350708\n* Loss at iteration 50 - train loss: 1.0523931980133057, test loss: 0.7053629159927368\n* Loss at iteration 100 - train loss: 0.6232627630233765, test loss: 0.6956414580345154\n* Loss at iteration 150 - train loss: 0.6970182061195374, test loss: 0.5816259384155273\n* Loss at iteration 200 - train loss: 0.5333189964294434, test loss: 0.49696922302246094\n* Loss at iteration 250 - train loss: 0.47810685634613037, test loss: 0.4219547510147095\n\n#### Model Performance \n##### Classification Report\n```\nprecision    recall  f1-score   support\n\n         cat       0.78      0.90      0.84       408\n         dog       0.88      0.74      0.80       392\n\n    accuracy                           0.82       800\n   macro avg       0.83      0.82      0.82       800\nweighted avg       0.83      0.82      0.82       800\n```\n\n","metadata":{}},{"cell_type":"markdown","source":"#### 2. CNN Base model - v2\n\n##### Saved file\n/kaggle/working/cnn-base-2.hdf5\n\n##### Dataset\n```\nIMAGE_DIM = 128\nCHANNEL_DIM = 3\nrequired_cat_count = 5000\nrequired_dog_count = 5000\n```\n##### Hyperparameters\n```\nITERATIONS = 10000\nLR = 1e-3\nWEIGHT_DECAY = 0.8\n```\n\n##### Architecture\n```\ntorch.manual_seed(1234)\nclass CNN(nn.Module):\n    def __init__(self, output_dim):\n        super(CNN, self).__init__()\n        self.feature_extraction_layers = nn.Sequential(nn.Conv2d(CHANNEL_DIM, 16, kernel_size= (3,3), padding=1), \n                                                       nn.BatchNorm2d(16), # Use batch normalization to speed up training and reduce overfitting \n                                                       nn.ReLU(), # Use ReLU activation function for faster convergence and sparsity \n                                                       nn.MaxPool2d((2,2) ,stride=2),\n                                                       \n                                                       nn.Conv2d(16, 32, kernel_size= (3,3), padding=1), \n                                                       nn.BatchNorm2d(32), nn.ReLU(), \n                                                       nn.MaxPool2d((2,2), stride=2), \n                                                       nn.Conv2d(32, 64, kernel_size= (3,3), padding=1),\n                                                       nn.BatchNorm2d(64), nn.ReLU(), \n                                                       nn.MaxPool2d((2,2), stride=2), \n                                                       nn.Conv2d(64, 128, kernel_size= (3,3), padding=1), \n                                                       nn.BatchNorm2d(128), nn.ReLU(), \n                                                       nn.MaxPool2d((2,2), stride=2),\n                                                       nn.Conv2d(128, 256, kernel_size= (3,3), padding=1), \n                                                       nn.BatchNorm2d(256), nn.ReLU(), \n                                                       nn.MaxPool2d((2,2), stride=2)) \n        self.flatten = nn.Flatten() \n        self.dropout = nn.Dropout(0.8) # Increase the dropout rate to prevent overfitting \n        self.connected_layer = nn.Linear(4096, 512) # Adjust the input dimension of the linear layer according to the output of the convolutional layers \n        self.output_layer = nn.Linear(512, output_dim)\n        \n    def forward(self, x):\n        feature_extraction_layer_output = self.feature_extraction_layers(x)\n        flatten_output = self.flatten(feature_extraction_layer_output)\n        connected_layer_output = self.connected_layer(self.dropout(flatten_output))\n        output = self.output_layer(self.dropout(connected_layer_output))\n        return output\n```\n#### Model Performance \n```Loss at iteration 0 - train loss: 1.236282467842102, test loss: 0.6929770708084106\nLoss at iteration 1000 - train loss: 0.5520414710044861, test loss: 0.773626446723938\nLoss at iteration 2000 - train loss: 0.18896639347076416, test loss: 0.44813066720962524\nLoss at iteration 3000 - train loss: 0.034101538360118866, test loss: 0.4923945367336273\nLoss at iteration 4000 - train loss: 0.016995254904031754, test loss: 0.3038991689682007\nLoss at iteration 5000 - train loss: 0.004320989828556776, test loss: 0.44383835792541504\nLoss at iteration 6000 - train loss: 0.021008003503084183, test loss: 1.594154715538025\nLoss at iteration 7000 - train loss: 0.001111862133257091, test loss: 0.45177677273750305\nLoss at iteration 8000 - train loss: 0.002191168488934636, test loss: 0.47085660696029663\nLoss at iteration 9000 - train loss: 0.006088821683079004, test loss: 0.34020543098449707\n```\n\n##### Classification Report\n\n```\n     precision    recall  f1-score   support\n\n         cat       0.89      0.96      0.93       988\n         dog       0.96      0.89      0.92      1012\n\n    accuracy                           0.93      2000\n    macro avg       0.93      0.93      0.93      2000\n    weighted avg       0.93      0.93      0.93      2000\n```\n\n","metadata":{"execution":{"iopub.status.busy":"2023-05-21T06:12:27.481040Z","iopub.execute_input":"2023-05-21T06:12:27.481472Z","iopub.status.idle":"2023-05-21T06:12:27.501302Z","shell.execute_reply.started":"2023-05-21T06:12:27.481432Z","shell.execute_reply":"2023-05-21T06:12:27.499888Z"}}},{"cell_type":"markdown","source":"#### 3. CNN Base model - v3\n\n##### Saved file\n/kaggle/working/cnn-base-3.hdf5\n\n##### Dataset\n```\nIMAGE_DIM = 128\nCHANNEL_DIM = 3\nrequired_cat_count = 12000\nrequired_dog_count = 12000\n```\n##### Hyperparameters\n```\nITERATIONS = 15000\nLR = 1e-4\nWEIGHT_DECAY = 3\nBATCH = 256\n```\n\n##### Architecture\n```\ntorch.manual_seed(1234)\nclass CNN(nn.Module):\n    def __init__(self, output_dim):\n        super(CNN, self).__init__()\n        self.feature_extraction_layers = nn.Sequential(nn.Conv2d(CHANNEL_DIM, 16, kernel_size= (3,3), padding=1), \n                                                       nn.BatchNorm2d(16), # Use batch normalization to speed up training and reduce overfitting \n                                                       nn.ReLU(), # Use ReLU activation function for faster convergence and sparsity \n                                                       nn.MaxPool2d((2,2) ,stride=2),\n                                                       \n                                                       nn.Conv2d(16, 32, kernel_size= (3,3), padding=1), \n                                                       nn.BatchNorm2d(32), nn.ReLU(), \n                                                       nn.MaxPool2d((2,2), stride=2), \n                                                       nn.Conv2d(32, 64, kernel_size= (3,3), padding=1),\n                                                       nn.BatchNorm2d(64), nn.ReLU(), \n                                                       nn.MaxPool2d((2,2), stride=2), \n                                                       nn.Conv2d(64, 128, kernel_size= (3,3), padding=1), \n                                                       nn.BatchNorm2d(128), nn.ReLU(), \n                                                       nn.MaxPool2d((2,2), stride=2),\n                                                       nn.Conv2d(128, 256, kernel_size= (3,3), padding=1), \n                                                       nn.BatchNorm2d(256), nn.ReLU(), \n                                                       nn.MaxPool2d((2,2), stride=2)) \n        self.flatten = nn.Flatten() \n        self.dropout = nn.Dropout(0.8) # Increase the dropout rate to prevent overfitting \n        self.connected_layer = nn.Linear(4096, 512) # Adjust the input dimension of the linear layer according to the output of the convolutional layers \n        self.output_layer = nn.Linear(512, output_dim)\n        \n    def forward(self, x):\n        feature_extraction_layer_output = self.feature_extraction_layers(x)\n        flatten_output = self.flatten(feature_extraction_layer_output)\n        connected_layer_output = self.connected_layer(self.dropout(flatten_output))\n        output = self.output_layer(self.dropout(connected_layer_output))\n        return output\n```\n#### Model Performance \n```\nLoss at iteration 0 - train loss: 1.080051302909851, test loss: 0.6915969848632812\nLoss at iteration 1000 - train loss: 0.33023184537887573, test loss: 0.5084758400917053\nLoss at iteration 2000 - train loss: 0.13062278926372528, test loss: 0.3834241032600403\nLoss at iteration 3000 - train loss: 0.12334740161895752, test loss: 0.6894320249557495\nLoss at iteration 4000 - train loss: 0.02116161584854126, test loss: 0.6645081043243408\nLoss at iteration 5000 - train loss: 0.016679178923368454, test loss: 0.3315171003341675\nLoss at iteration 6000 - train loss: 0.00930651556700468, test loss: 0.490634024143219\nLoss at iteration 7000 - train loss: 0.03066173940896988, test loss: 0.5103933811187744\nLoss at iteration 8000 - train loss: 0.007853752002120018, test loss: 0.7809205055236816\nLoss at iteration 9000 - train loss: 0.04008251801133156, test loss: 4.434248924255371\nLoss at iteration 10000 - train loss: 0.0021463362500071526, test loss: 0.23868006467819214\nLoss at iteration 11000 - train loss: 0.016685249283909798, test loss: 0.8665417432785034\nLoss at iteration 12000 - train loss: 0.010997233912348747, test loss: 0.31216633319854736\nLoss at iteration 13000 - train loss: 0.0020246654748916626, test loss: 0.33097776770591736\nLoss at iteration 14000 - train loss: 0.002831029240041971, test loss: 0.2651127278804779\n```\n\n##### Classification Report\n\n```\nprecision    recall  f1-score   support\n\n         cat       0.98      0.99      0.98      2317\n         dog       0.99      0.98      0.98      2483\n\n    accuracy                           0.98      4800\n   macro avg       0.98      0.98      0.98      4800\nweighted avg       0.98      0.98      0.98      4800\n```\n\n","metadata":{}},{"cell_type":"markdown","source":"cnn.eval()\ntorch.save(cnn, '/kaggle/working/cnn-base-3.hdf5')","metadata":{"execution":{"iopub.status.busy":"2023-05-21T12:59:17.843615Z","iopub.execute_input":"2023-05-21T12:59:17.844405Z","iopub.status.idle":"2023-05-21T12:59:17.881588Z","shell.execute_reply.started":"2023-05-21T12:59:17.844366Z","shell.execute_reply":"2023-05-21T12:59:17.880484Z"}}},{"cell_type":"code","source":"del X_train\ndel y_train\ntorch.cuda.empty_cache()","metadata":{"execution":{"iopub.status.busy":"2023-05-22T02:28:44.577449Z","iopub.execute_input":"2023-05-22T02:28:44.577824Z","iopub.status.idle":"2023-05-22T02:28:44.590700Z","shell.execute_reply.started":"2023-05-22T02:28:44.577786Z","shell.execute_reply":"2023-05-22T02:28:44.589693Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"saved_model = torch.load('/kaggle/working/cnn-base-3.hdf5')","metadata":{"execution":{"iopub.status.busy":"2023-05-22T02:28:44.592297Z","iopub.execute_input":"2023-05-22T02:28:44.592708Z","iopub.status.idle":"2023-05-22T02:28:44.648309Z","shell.execute_reply.started":"2023-05-22T02:28:44.592670Z","shell.execute_reply":"2023-05-22T02:28:44.647315Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import classification_report\nsaved_model.eval()\ny_prednumpy = np.array([])\ny_testnumpy = np.array([])\nfor i in range(0, X_test.shape[0], BATCH):\n    X_testb, y_testb = X_test[i: i + BATCH], y_test[i: i + BATCH]\n    logits_testb = saved_model(X_testb)\n    probs_testb = torch.sigmoid(logits_testb)\n    probs_test_numpy = probs_testb.cpu().detach().numpy().reshape(-1,)\n    probs_test_numpy[probs_test_numpy >= 0.5] = 1\n    probs_test_numpy[probs_test_numpy < 0.5] = 0\n    y_prednumpy = np.concatenate([y_prednumpy, probs_test_numpy], 0)\n    y_testnumpy = np.concatenate([y_testnumpy, y_testb.cpu().detach().numpy()], 0)","metadata":{"execution":{"iopub.status.busy":"2023-05-22T02:34:54.388795Z","iopub.execute_input":"2023-05-22T02:34:54.389525Z","iopub.status.idle":"2023-05-22T02:34:54.847581Z","shell.execute_reply.started":"2023-05-22T02:34:54.389483Z","shell.execute_reply":"2023-05-22T02:34:54.846525Z"},"trusted":true},"execution_count":32,"outputs":[]},{"cell_type":"code","source":"y_prednumpy.shape, y_testnumpy.shape","metadata":{"execution":{"iopub.status.busy":"2023-05-22T02:34:56.996290Z","iopub.execute_input":"2023-05-22T02:34:56.997272Z","iopub.status.idle":"2023-05-22T02:34:57.004612Z","shell.execute_reply.started":"2023-05-22T02:34:56.997214Z","shell.execute_reply":"2023-05-22T02:34:57.003449Z"},"trusted":true},"execution_count":33,"outputs":[{"execution_count":33,"output_type":"execute_result","data":{"text/plain":"((4800,), (4800,))"},"metadata":{}}]},{"cell_type":"code","source":"print(classification_report(y_prednumpy, y_testnumpy, target_names = ['cat', 'dog']))","metadata":{"execution":{"iopub.status.busy":"2023-05-22T02:35:11.229774Z","iopub.execute_input":"2023-05-22T02:35:11.230191Z","iopub.status.idle":"2023-05-22T02:35:11.253509Z","shell.execute_reply.started":"2023-05-22T02:35:11.230151Z","shell.execute_reply":"2023-05-22T02:35:11.252415Z"},"trusted":true},"execution_count":34,"outputs":[{"name":"stdout","text":"              precision    recall  f1-score   support\n\n         cat       0.98      0.99      0.98      2317\n         dog       0.99      0.98      0.98      2483\n\n    accuracy                           0.98      4800\n   macro avg       0.98      0.98      0.98      4800\nweighted avg       0.98      0.98      0.98      4800\n\n","output_type":"stream"}]},{"cell_type":"markdown","source":"cnn.eval()\nlogits_test = cnn(X_test)\nprobs_test = torch.sigmoid(logits_test)\nfrom torchmetrics import Accuracy\naccuracy = Accuracy(task = 'binary').to(device)","metadata":{"execution":{"iopub.status.busy":"2023-05-21T06:07:22.347531Z","iopub.execute_input":"2023-05-21T06:07:22.347929Z","iopub.status.idle":"2023-05-21T06:07:32.756104Z","shell.execute_reply.started":"2023-05-21T06:07:22.347892Z","shell.execute_reply":"2023-05-21T06:07:32.754791Z"}}}]}